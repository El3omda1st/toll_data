# ETL Toll Data Pipeline with Apache Airflow

This project implements an end-to-end ETL pipeline using **Apache Airflow** to automate the ingestion, transformation, and consolidation of toll data from multiple file formats. It demonstrates how to orchestrate data workflows using Airflow operators, task dependencies, and Python functions.

---

## ğŸš€ Project Overview

The pipeline performs the following tasks:

1. **Download** a compressed data archive containing CSV, TSV, and fixed-width files.
2. **Extract** relevant columns from each file format:
   - `CSV`: Vehicle data
   - `TSV`: Toll plaza data
   - `Fixed-width`: Payment data
3. **Consolidate** all extracted data into a single dataset.
4. **Transform** the data by converting vehicle types to uppercase.
5. Save intermediate and final outputs into a `staging/` directory.

---

## ğŸ› ï¸ Technologies Used

- **Apache Airflow** (via Docker)
- **Python** (pandas, os)
- **BashOperator & PythonOperator**
- **Pandas** for data processing
- **Docker & Docker Compose** for environment setup

---

## ğŸ—‚ï¸ Project Structure

```
airflow_docker_train/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ ETL_toll_data.py         # Main Airflow DAG
â”‚   â””â”€â”€ python_etl/
â”‚       â”œâ”€â”€ staging/             # Contains raw, intermediate, and output CSVs
â”‚       â”‚   â”œâ”€â”€ csv_data.csv
â”‚       â”‚   â”œâ”€â”€ tsv_data.csv
â”‚       â”‚   â”œâ”€â”€ fixed_width_data.csv
â”‚       â”‚   â”œâ”€â”€ extracted_data.csv
â”‚       â”‚   â””â”€â”€ transformed_data.csv
â”‚       â””â”€â”€ DWH/
â”‚           â””â”€â”€ Create_Tables.sql  # (Optional) Schema SQL file
â”‚
â”œâ”€â”€ docker-compose.yaml          # Docker setup for Airflow
â”œâ”€â”€ plugins/                     # (Empty or custom Airflow plugins)
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ“… DAG Overview

The DAG is scheduled to run **daily** and consists of the following tasks:

```
download_data
      â†“
  unzip_data
      â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚extract_csv â”‚ extract_tsv â”‚ extract_fixedâ”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“              â†“
     consolidate_data
              â†“
       transform_data
```

---

## âš™ï¸ How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/airflow_docker_train.git
cd airflow_docker_train
```

### 2. Start Airflow Using Docker Compose

```bash
docker-compose up --build
```

### 3. Access the Airflow Web UI

- Navigate to [http://localhost:8080](http://localhost:8080)
- Login with the default credentials (usually `airflow` / `airflow`)
- Enable and trigger the `ETL_toll_data` DAG

---

## âœ… Output

The final cleaned and transformed data is saved as:

```bash
/opt/airflow/dags/python_etl/staging/transformed_data.csv
```

---


## ğŸ“ License

This project is for educational purposes as part of the **IBM Data Engineering Capstone**.
